import sys
import os
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from service.ChatGPT import chat_simple as chat_simple_service

_SCORE_SYSTEM_PROMPT = """
You are a precise scorer. I will provide you with a professional peer review evaluation of an academic idea,
and you need to give a feasibility score based on this evaluation. Feasibility means whether the idea is easy to implement and execute and whether the idea is effective.
The feasibility score depends on how feasible and executable the idea is according to the evaluation.
Please note that the feasibility score ranges from 1 to 10, where 1 indicates the lowest feasibility and 10 indicates the highest feasibility.

IMPORTANT: The score should reflect the degree of feasibility and effectiveness as demonstrated in the evaluation text. You should judge the score based on:
1. The overall tone and assessment: Positive evaluations indicating high feasibility and effectiveness should lead to higher scores, while negative evaluations indicating significant challenges should lead to lower scores.
2. Implementation feasibility: Comments about ease of implementation, straightforward methods, available datasets, and clear execution plans should increase the score.
3. Effectiveness concerns: Concerns about whether the method will work, comparisons with baselines, and potential limitations should decrease the score.
4. Resource requirements: Comments about computational requirements, data collection challenges, and manual efforts should decrease the score.

Here are some examples based on real evaluation data from Stanford_comments_with_ideas_with_scores.json with full review comments (all_comments) and their corresponding average scores:

Example 1 (Low-medium score case, score 4.5):
Evaluation: "The hardest part is data curation, but the evaluation set used in the paper would be a valuable contribution to the literature. The scope of the project needs to be clearly defined so that only a specific kind of \"vernacular\" is being studied, otherwise collecting data will be tough. I just don't think frontier LLMs have any trouble with the kind of modern language shown in the examples. Furthermore, we can't reliably predict semantic change so I don't think this will even work if e.g. you use a 2020-trained LM on language from 2040. The only solution would be some kind of RAG, expert prompting, or continued pretraining. Making the model guess what semantic change has happened is just not going to work. I am not sure how hard it is to construct such a graph. But it seems to be a quite straightforward method to implement. The experimental setup and experiments does not seem to be difficulty to manage as well. I don't feel this will work well compared to just include the explanations of the phrases in prompts."
Analysis: The evaluation acknowledges some positive aspects (evaluation set contribution, straightforward implementation) but identifies significant challenges: unclear scope definition, difficult data collection, fundamental limitations in predicting semantic change, and doubts about the core method's effectiveness. These concerns indicate moderate feasibility with substantial challenges.
Score: 4.5

Example 2 (Medium score case, score 5.0):
Evaluation: "I imagine step (2) in the proposed idea would be a little challenging in execution. Manual efforts would perhaps be involved to generate the 'polarity reversed' world descriptions, or a combination of automation and manual validation would be required to ensure the quality of these descriptions. Also, the example illustrated in the proposed idea does not come from the datasets mentioned in Step 1. This will likely cause extra planning steps to finalize how the prompting technique would be applied to each of the individual datasets. The example shown in the proposed idea already has a strong baseline that does not show explicit gender stereotype. In contrast, the proposed method could offer counter-factual explanations or arguments because of the \"stereotype inversion.\" This makes me feel the proposed idea does not address the fairness problem in LLMs better than existing safety guardrails. Very easy to implement: requires no model training, datasets are widely available. The prompts seem easy to template as well. I would expect this to work reasonably well since a variety of prompt-based approaches have been shown to work well on this sort of task. However, I'm skeptical that this prompt-based approach would significantly outperform existing prompt-based approaches. It shouldn't be hard to implement this ideas since it is very straightforward. This idea does not make sense to me at all. Why would reversing the stereotype could help to reduce the bias? It simply creates another kinds of bias."
Analysis: The evaluation identifies specific execution challenges, including manual work requirements and dataset alignment issues. There are also concerns about the method's effectiveness compared to existing approaches. However, it acknowledges that implementation is straightforward with available datasets. These factors indicate moderate feasibility with some concerns about both implementation and effectiveness.
Score: 5.0

Example 3 (Medium score case, score 5.25):
Evaluation: "The significant feasibility problem for this proposal is it depends too much on the performance of the proposed method, and in the field of uncertainty quantification, due to the black-box nature of model (the usage of GPT-3.5/4 gets this situation even worse), it is hard to say whether we can get an ideal uncertainty measurement that can perform well in the evaluation setup proposed. So there might be many re-routings, and it is hard to tell whether any of them would work. The big issue for the effectiveness of the proposed method is that, it asserts very strong assumptions on downstream tasks, such as there must exist only two extremes, or at least two extremes are complicated enough for quantifying uncertainty. This is definitely not true. Think about multi-choice QA, emotion detection, etc. In these tasks, there are far more than two extremes, and even a spectrum of extremes. We also do not know how the model understand the task -- so it might be unfair for the model uncertainty quantification by using human priors that there are only two extremes. Also, this proposal assumes that the model can place its own output under two extremes well -- it is hard to say, as it may not even understand what it generates (\"Generative AI paradox\"). Most of the proposal seems straightforward and quick to execute (straightforward prompting and some generally simple analysis). I'm docking feasibility since they mention wanting to compare to human uncertainty ratings (which, as proposed, seems to be of limited utility, but that's besides the point). Depending on how they'd go about this, involving humans could significantly increase their timeline. I could be misunderstanding, but I have a hard time imagining this would work well. It seems like the poles will only be useful if they're actually relevant to the question and if they encourage the model to use a piece of information that it actually \"knows\" and isn't already relying on. In their Paris example, being a \"major global city\" may be a good sign that a city is a capital, but would a model that doesn't \"know\" that Paris is a capital \"know\" that it is a major city? I'd be concerned that the model's ability to pick a good axis and to put their answer on the axis would correlate with the model's ability to \"understand\" the scenario and quantify its uncertainty in the baseline setting. It also seems like multiple axes/poles would be necessary to really get a good sense of the answer. London is also a \"major global city\" but clearly not the capital of France."
Analysis: The evaluation identifies significant feasibility and effectiveness concerns. While the basic proposal seems straightforward to execute, there are fundamental doubts about whether the method can work well, with concerns about strong assumptions, model understanding, and the core approach. These factors indicate moderate feasibility with substantial effectiveness concerns.
Score: 5.25

Example 4 (Medium-high score case, score 6.17):
Evaluation: "The structure is clear and the implementation of the LLM pipeline is not very heavy. One caveat is that in the proposal it mentioned a specialist is needed to manually verify if the identified defect is actually an issue in the SRS document. The domain-specific expertise required can make it less feasible for a typical CS PhD not in the domain. I think by breaking down a long document into sections, and focusing on each of the independent sections separately, the method will highly likely decrease the difficulty for LLMs to understand the texts (as there are less concepts, relationship, etc). And it can be expected to see a positive improvement on the tasks. That being said, recently released LLMs have very long context lengths. For example Claude-3.5-sonnet has 200K context lengths. I wonder how big the difference will be if we just input the whole document, generate the defect questions, and verify them. The approach itself is fairly straightforward. You can draw upon existing synthetic pipeline approaches and even reuse existing codebases towards SRS. The idea itself is fairly likely to succeed. The application space is constrained towards SRS and each of the sections within the document. Therefore, the LM can likely handle documents and specifics of the domain through in-context learning or fine-tuning. The idea is straightforward to implement. Since they have have abundant OpenAI / Anthropic API access, generating queries on defects with LLMs is not a problem. The only potential difficulty is whether there are enough human resources to label the ground truths. It may be a bit challenging to find people with sufficient expertise on SRS to do the labeling. Converting SRS Document defects detection prompts into yes/no questions on sections of the document does not fundamentally change the way of applying LLMs on this problem. It is too simple and does not have any specific designs such as planning or states tracking or finetuning to enhance LLMs ability. Constructing the questions by sections is also unlikely to work better than existing conventional approaches like RAG. Overall, this idea is unlikely to work well, even in the specific scenarios of SRS Document defects detection."
Analysis: The evaluation indicates the basic components are feasible (clear structure, straightforward implementation, available APIs), but raises concerns about specialist requirements, potential limitations compared to simpler approaches, and doubts about effectiveness. Overall, the project seems feasible but with uncertainty about effectiveness.
Score: 6.17

Example 5 (High score case, score 7.0):
Evaluation: "feasible as only tracking the intermediate status. intuitively thinking this should works. The experiments are fairly easy to implement. They can be broken into separate components around tool use (e.g. compilers), multi-turn reasoning, unit test generation and evaluation, and more. Most code approaches right now also use multi-turn approaches and unit tests. However, not many of them integrate tool use and, when they do, it isn't that good. By make it iterative and allowing the models to \"hill climb\" through the use of a continuous state, it could significantly improve performance. This seems to be a prompting focused project and therefore shouldn't require more intricate code like model training. I therefore think that one to two months is a very feasible timeline. Based on the related paper I cited above, I wouldn't be surprised if this method improved performance on several benchmarks. However, I'd expect other methods, like actually executing the code instead of having the LLM simulate the execution, might work much better."
Analysis: The evaluation indicates the method is feasible with easy implementation, clear components, and a reasonable timeline. While there are some concerns about effectiveness compared to other methods, the overall feasibility is high with positive expectations about performance improvements.
Score: 7.0

Example 6 (High score case, score 7.25):
Evaluation: "No issue with the execution. A lot of previous work have approached this problem in a very similar way, so it would have similar performance. Additionally, if a model hallucinates on the CoT step, evaluating each CoT might also hallucinate, which is harmful to entire pipeline This is a highly feasible experiment especially with the existing dataset and the existing approach already been completed by another paper and so I do think it is highly feasible and straightforward to implement the idea and run the experiments. The idea should somewhat work because as it has been shown in the previous paper the idea does somewhat work but it suffers from the same issue as do things like multiple looping structures where if we can't verify whether the first three strings that are generated make sense or not then the chain of thought wouldn't necessarily make sense because it is all interlinked in a loop like structure even if it has three loops that are running concurrently with each other."
Analysis: The evaluation explicitly states there are no execution issues and that the experiment is highly feasible with existing datasets and approaches. While there are some concerns about hallucination and verification, these are effectiveness concerns rather than feasibility barriers. The project is clearly executable with positive expectations.
Score: 7.25

"""

def generate_feasibility_score(evaluation_text: str) -> str:
    prompt = (
        _SCORE_SYSTEM_PROMPT
        + "\n\n"
        + "So now please start scoring formally: give me a score between 1 and 10 based on the following peer review evaluation: \n\n"
        + "Evaluation: " + evaluation_text.strip()
        + "\n\nPlease return an analysis of the evaluation and the final scoring result. "
        + "IMPORTANT: You must format your response with the score clearly marked at the end. "
        + "Use the exact format: 'Score: X' where X is a number between 1 and 10 (can be a decimal like 4.5, 7.75, etc.). "
        + "For example, if your score is 7, end your response with 'Score: 7'. If your score is 7.5, end with 'Score: 7.5'."
    )
    try:
        content = chat_simple_service(prompt)
    except Exception as e:
        return f"ERROR: {e}"

    return content
