#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Evaluate the feasibility score of concerns generated by RAG model
Read gen_concerns from Evaluation_feasibility results, convert to text format, then score using eval_feasibility_score.py
"""

import sys
import os
import json
import re
import time
from multiprocessing import Pool, cpu_count
from typing import Dict, List, Tuple, Optional

import numpy as np
from scipy.stats import pearsonr, spearmanr
from sklearn.metrics import cohen_kappa_score

# Ensure modules from project root can be imported
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from Evaluation_utils.eval_feasibility_score import generate_feasibility_score
from RAG_single import generate_feasibility_evaluation
from RAG_baseline_review_sentence.retrieval_system import EvidenceRetrievalSystem

# Global retrieval system variable for multiprocessing
global_retrieval_system = None

def init_worker():
    """Initialize worker process, create retrieval system"""
    global global_retrieval_system
    embeddings_dir = 'RAG_baseline_review_sentence'
    global_retrieval_system = EvidenceRetrievalSystem(embeddings_dir)


def extract_score_from_response(response: str) -> Optional[float]:
    """
    Extract score from GPT response
    Supports integer and decimal scores
    """
    if not response or response.startswith("ERROR"):
        return None
    
    response = response.strip()
    lines = response.split('\n')
    last_lines = '\n'.join(lines[-5:]) if len(lines) > 5 else response
    last_lines = last_lines.strip()
    
    score_patterns = [
        r'^Score\s*:\s*(\d+(?:\.\d+)?)$',
        r'\nScore\s*:\s*(\d+(?:\.\d+)?)\s*$',
        r'Score\s*:\s*(\d+(?:\.\d+)?)',
        r'分数\s*:\s*(\d+(?:\.\d+)?)',
        r'feasibility\s*score\s*:\s*(\d+(?:\.\d+)?)',
        r'可行性分数\s*:\s*(\d+(?:\.\d+)?)',
        r'评分\s*:\s*(\d+(?:\.\d+)?)',
    ]
    
    for pattern in score_patterns:
        match = re.search(pattern, last_lines, re.MULTILINE | re.IGNORECASE)
        if match:
            score = float(match.group(1))
            if 1.0 <= score <= 10.0:
                return score
    
    for pattern in score_patterns:
        match = re.search(pattern, response, re.MULTILINE | re.IGNORECASE)
        if match:
            score = float(match.group(1))
            if 1.0 <= score <= 10.0:
                return score
    
    score_10_pattern = r'(\d+(?:\.\d+)?)\s*/\s*10'
    match = re.search(score_10_pattern, response, re.IGNORECASE)
    if match:
        score = float(match.group(1))
        if 1.0 <= score <= 10.0:
            return score
    
    last_few_lines = '\n'.join(lines[-3:]) if len(lines) > 3 else response
    numbers = re.findall(r'\b(\d+(?:\.\d+)?)\b', last_few_lines)
    if numbers:
        for num_str in reversed(numbers):
            score = float(num_str)
            if 1.0 <= score <= 10.0:
                return score
    
    all_numbers = re.findall(r'\b(\d+(?:\.\d+)?)\b', response)
    if all_numbers:
        for num_str in reversed(all_numbers):
            score = float(num_str)
            if 1.0 <= score <= 10.0:
                return score
    
    return None


def process_single_sample(args: Tuple[Dict, Dict[str, float]]) -> Tuple[str, Optional[float], Optional[float], Optional[str], Optional[str]]:
    """
    Process a single sample
    Args: (sample, golden_scores)
    Returns: (sample_id, golden_score, predicted_score, response_text, evaluation_text)
    """
    global global_retrieval_system
    sample, golden_scores = args
    sample_id = sample.get('id', 'unknown')
    idea_text = sample.get('idea', '')
    golden_score = golden_scores.get(sample_id, None)
    
    if not idea_text or golden_score is None:
        return (sample_id, golden_score, None, None, None)
    
    try:
        # Use global retrieval system (each process has its own instance)
        embeddings_dir = 'RAG_baseline_review_sentence'
        # Generate feasibility evaluation text (similar to all_comments)
        evaluation_text = generate_feasibility_evaluation(idea_text, embeddings_dir, global_retrieval_system)
        
        if not evaluation_text or evaluation_text.startswith("ERROR"):
            return (sample_id, golden_score, None, None, evaluation_text)
        
        # Directly use evaluation text to call scoring function
        response = generate_feasibility_score(evaluation_text)
        predicted_score = extract_score_from_response(response)
        return (sample_id, golden_score, predicted_score, response, evaluation_text)
    except Exception as e:
        print(f"Error processing sample {sample_id}: {e}")
        return (sample_id, golden_score, None, str(e), None)


def calculate_metrics(true_scores: List[float], predicted_scores: List[float]) -> Dict:
    """
    Calculate evaluation metrics
    """
    valid_indices = [i for i, (t, p) in enumerate(zip(true_scores, predicted_scores)) 
                     if t is not None and p is not None]
    
    if len(valid_indices) == 0:
        return {
            'accuracy_exact': 0.0,
            'accuracy_within_0.5': 0.0,
            'accuracy_within_1': 0.0,
            'accuracy_within_1.5': 0.0,
            'accuracy_within_2': 0.0,
            'pearson_correlation': 0.0,
            'spearman_correlation': 0.0,
            'cohen_kappa': 0.0,
            'valid_samples': 0,
            'total_samples': len(true_scores)
        }
    
    true_valid = [true_scores[i] for i in valid_indices]
    pred_valid = [predicted_scores[i] for i in valid_indices]
    
    # Accuracy calculation
    exact_matches = sum(1 for t, p in zip(true_valid, pred_valid) if abs(t - p) < 0.1)
    accuracy_exact = exact_matches / len(true_valid)
    
    within_0_5_matches = sum(1 for t, p in zip(true_valid, pred_valid) if abs(t - p) <= 0.5)
    accuracy_within_0_5 = within_0_5_matches / len(true_valid)
    
    within_1_matches = sum(1 for t, p in zip(true_valid, pred_valid) if abs(t - p) <= 1.0)
    accuracy_within_1 = within_1_matches / len(true_valid)
    
    within_1_5_matches = sum(1 for t, p in zip(true_valid, pred_valid) if abs(t - p) <= 1.5)
    accuracy_within_1_5 = within_1_5_matches / len(true_valid)
    
    within_2_matches = sum(1 for t, p in zip(true_valid, pred_valid) if abs(t - p) <= 2.0)
    accuracy_within_2 = within_2_matches / len(true_valid)
    
    # Correlation coefficients
    pearson_r, pearson_p = pearsonr(true_valid, pred_valid)
    spearman_r, spearman_p = spearmanr(true_valid, pred_valid)
    
    # Cohen's Kappa
    true_rounded = [round(t) for t in true_valid]
    pred_rounded = [round(p) for p in pred_valid]
    cohen_kappa = cohen_kappa_score(true_rounded, pred_rounded, weights='quadratic')
    
    return {
        'accuracy_exact': accuracy_exact,
        'accuracy_within_0.5': accuracy_within_0_5,
        'accuracy_within_1': accuracy_within_1,
        'accuracy_within_1.5': accuracy_within_1_5,
        'accuracy_within_2': accuracy_within_2,
        'pearson_correlation': pearson_r,
        'pearson_p_value': pearson_p,
        'spearman_correlation': spearman_r,
        'spearman_p_value': spearman_p,
        'cohen_kappa': cohen_kappa,
        'valid_samples': len(true_valid),
        'total_samples': len(true_scores),
        'mean_absolute_error': np.mean([abs(t - p) for t, p in zip(true_valid, pred_valid)]),
        'rmse': np.sqrt(np.mean([(t - p) ** 2 for t, p in zip(true_valid, pred_valid)])),
        'mean_true_score': np.mean(true_valid),
        'mean_predicted_score': np.mean(pred_valid),
        'std_true_score': np.std(true_valid),
        'std_predicted_score': np.std(pred_valid)
    }


def load_golden_scores() -> Dict[str, float]:
    """
    Load golden scores from Stanford_comments_with_ideas_with_scores.json
    """
    golden_file = os.path.join(PROJECT_ROOT, 'Evaluation_feasibility_score', 
                              'Stanford_comments_with_ideas_with_scores.json')
    
    with open(golden_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    golden_scores = {}
    for item in data:
        sample_id = item.get('id')
        average_score = item.get('average_score')
        if sample_id and average_score is not None:
            golden_scores[sample_id] = average_score
    
    return golden_scores


def main():
    """Main function"""
    print("=" * 60)
    print("Evaluate the feasibility score of concerns generated by RAG model")
    print("=" * 60)
    
    # Load golden scores
    print("\nLoading golden scores...")
    golden_scores = load_golden_scores()
    print(f"Loaded {len(golden_scores)} golden scores")
    
    # Load input data
    input_file = os.path.join(PROJECT_ROOT, 'Evaluation_feasibility_score', 
                              'Stanford_comments_with_ideas_with_scores.json')
    print(f"\nLoading input data file: {input_file}")
    
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"Total samples: {len(data)}")
    
    # Filter valid samples (need idea and golden score)
    valid_data = [item for item in data 
                  if item.get('idea') and item.get('id') in golden_scores]
    print(f"Valid samples: {len(valid_data)}")
    
    if len(valid_data) == 0:
        print("Error: No valid samples!")
        return
    
    # Determine number of processes (refer to Evaluation_feasibility/RAG_batch.py, use smaller number of processes)
    num_processes = 5
    print(f"Using {num_processes} processes for parallel processing")
    
    # Process in batches to avoid API rate limiting
    batch_size = num_processes * 2
    all_results = []
    start_time = time.time()
    
    for i in range(0, len(valid_data), batch_size):
        batch = valid_data[i:i + batch_size]
        batch_num = i // batch_size + 1
        total_batches = (len(valid_data) + batch_size - 1) // batch_size
        
        print(f"\nProcessing batch {batch_num}/{total_batches} (samples {i+1}-{min(i+batch_size, len(valid_data))})")
        
        # Use multiprocessing to process batch (use initializer to initialize retrieval system for each process)
        with Pool(processes=num_processes, initializer=init_worker) as pool:
            batch_results = pool.map(process_single_sample, [(sample, golden_scores) for sample in batch])
        
        all_results.extend(batch_results)
        
        # Display progress
        elapsed = time.time() - start_time
        processed = i + len(batch)
        if processed > 0:
            avg_time_per_sample = elapsed / processed
            remaining_samples = len(valid_data) - processed
            estimated_remaining = avg_time_per_sample * remaining_samples
            
            print(f"  Completed: {processed}/{len(valid_data)}")
            print(f"  Elapsed time: {elapsed:.1f} seconds")
            print(f"  Estimated remaining time: {estimated_remaining:.1f} seconds")
        
        # Avoid API rate limiting, rest briefly
        if i + batch_size < len(valid_data):
            time.sleep(1)
    
    total_time = time.time() - start_time
    print(f"\nProcessing completed! Total time: {total_time:.1f} seconds")
    
    # Extract results
    sample_ids = [r[0] for r in all_results]
    true_scores = [r[1] for r in all_results]
    predicted_scores = [r[2] for r in all_results]
    responses = [r[3] for r in all_results]
    evaluation_texts = [r[4] for r in all_results]
    
    # Statistics
    valid_predictions = sum(1 for p in predicted_scores if p is not None)
    print(f"Valid predictions: {valid_predictions}/{len(all_results)}")
    
    # Calculate metrics
    print("\nCalculating evaluation metrics...")
    metrics = calculate_metrics(true_scores, predicted_scores)
    metrics['total_time_seconds'] = total_time
    
    # Print results
    print("\n" + "=" * 60)
    print("Evaluation Results")
    print("=" * 60)
    print(f"Total samples: {metrics['total_samples']}")
    print(f"Valid predictions: {metrics['valid_samples']}")
    print(f"\nAccuracy (exact match, ±0.1 error): {metrics['accuracy_exact']:.4f} ({metrics['accuracy_exact']*100:.2f}%)")
    print(f"Accuracy (±0.5 error): {metrics['accuracy_within_0.5']:.4f} ({metrics['accuracy_within_0.5']*100:.2f}%)")
    print(f"Accuracy (±1.0 error): {metrics['accuracy_within_1']:.4f} ({metrics['accuracy_within_1']*100:.2f}%)")
    print(f"Accuracy (±1.5 error): {metrics['accuracy_within_1.5']:.4f} ({metrics['accuracy_within_1.5']*100:.2f}%)")
    print(f"Accuracy (±2.0 error): {metrics['accuracy_within_2']:.4f} ({metrics['accuracy_within_2']*100:.2f}%)")
    print(f"\nPearson correlation: {metrics['pearson_correlation']:.4f} (p={metrics['pearson_p_value']:.4f})")
    print(f"Spearman correlation: {metrics['spearman_correlation']:.4f} (p={metrics['spearman_p_value']:.4f})")
    print(f"Cohen's Kappa: {metrics['cohen_kappa']:.4f}")
    print(f"\nMean Absolute Error (MAE): {metrics['mean_absolute_error']:.4f}")
    print(f"Root Mean Square Error (RMSE): {metrics['rmse']:.4f}")
    print(f"\nTrue score mean: {metrics['mean_true_score']:.4f} (std: {metrics['std_true_score']:.4f})")
    print(f"Predicted score mean: {metrics['mean_predicted_score']:.4f} (std: {metrics['std_predicted_score']:.4f})")
    
    # Save results
    results_dir = os.path.join(PROJECT_ROOT, "Evaluation_feasibility_score", "results", "RAG_evidence_retrieval")
    os.makedirs(results_dir, exist_ok=True)
    
    results_file = os.path.join(results_dir, 'feasibility_score_result.json')
    
    # Prepare data to save
    results_data = {
        'metrics': metrics,
        'total_time_seconds': total_time,
        'samples': [
            {
                'id': sid,
                'golden_score': ts,
                'predicted_score': ps,
                'error': abs(ts - ps) if ts is not None and ps is not None else None,
                'response': resp,
                'evaluation_text': eval_text
            }
            for sid, ts, ps, resp, eval_text in zip(sample_ids, true_scores, predicted_scores, responses, evaluation_texts)
        ]
    }
    
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nResults saved to: {results_file}")
    
    # Save detailed report
    report_file = os.path.join(results_dir, 'feasibility_score_result_report.txt')
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("=" * 60 + "\n")
        f.write("RAG Model Feasibility Score Evaluation Report\n")
        f.write("=" * 60 + "\n\n")
        f.write(f"Test time: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Total samples: {metrics['total_samples']}\n")
        f.write(f"Valid predictions: {metrics['valid_samples']}\n")
        f.write(f"Total time: {total_time:.1f} seconds\n\n")
        
        f.write("Evaluation Metrics:\n")
        f.write(f"  Accuracy (exact match, ±0.1 error): {metrics['accuracy_exact']:.4f} ({metrics['accuracy_exact']*100:.2f}%)\n")
        f.write(f"  Accuracy (±0.5 error): {metrics['accuracy_within_0.5']:.4f} ({metrics['accuracy_within_0.5']*100:.2f}%)\n")
        f.write(f"  Accuracy (±1.0 error): {metrics['accuracy_within_1']:.4f} ({metrics['accuracy_within_1']*100:.2f}%)\n")
        f.write(f"  Accuracy (±1.5 error): {metrics['accuracy_within_1.5']:.4f} ({metrics['accuracy_within_1.5']*100:.2f}%)\n")
        f.write(f"  Accuracy (±2.0 error): {metrics['accuracy_within_2']:.4f} ({metrics['accuracy_within_2']*100:.2f}%)\n")
        f.write(f"  Pearson correlation: {metrics['pearson_correlation']:.4f} (p={metrics['pearson_p_value']:.4f})\n")
        f.write(f"  Spearman correlation: {metrics['spearman_correlation']:.4f} (p={metrics['spearman_p_value']:.4f})\n")
        f.write(f"  Cohen's Kappa: {metrics['cohen_kappa']:.4f}\n")
        f.write(f"  Mean Absolute Error (MAE): {metrics['mean_absolute_error']:.4f}\n")
        f.write(f"  Root Mean Square Error (RMSE): {metrics['rmse']:.4f}\n")
        f.write(f"  True score mean: {metrics['mean_true_score']:.4f} (std: {metrics['std_true_score']:.4f})\n")
        f.write(f"  Predicted score mean: {metrics['mean_predicted_score']:.4f} (std: {metrics['std_predicted_score']:.4f})\n\n")
        
        f.write("\n" + "=" * 60 + "\n")
        f.write("Detailed Prediction Results\n")
        f.write("=" * 60 + "\n\n")
        
        for i, (sid, ts, ps, resp, eval_text) in enumerate(zip(sample_ids, true_scores, predicted_scores, responses, evaluation_texts)):
            f.write(f"Sample {i+1}: {sid}\n")
            f.write(f"  True score: {ts if ts is not None else 'None'}\n")
            f.write(f"  Predicted score: {ps if ps is not None else 'None'}\n")
            if ts is not None and ps is not None:
                f.write(f"  Error: {abs(ts - ps):.2f}\n")
            if eval_text:
                f.write(f"  Evaluation text: {eval_text[:300]}...\n" if len(eval_text) > 300 else f"  Evaluation text: {eval_text}\n")
            if resp:
                f.write(f"  Scoring response: {resp[:200]}...\n" if len(resp) > 200 else f"  Scoring response: {resp}\n")
            f.write("\n")
    
    print(f"Detailed report saved to: {report_file}")
    
    print("\n" + "=" * 60)
    print("Evaluation completed!")
    print("=" * 60)


if __name__ == "__main__":
    main()

